{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b88d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b2d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:/DataSets ML/tumorData/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca54ba1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0    ...          17.33           184.60      2019.0           0.16220   \n",
       "1    ...          23.41           158.80      1956.0           0.12380   \n",
       "2    ...          25.53           152.50      1709.0           0.14440   \n",
       "3    ...          26.50            98.87       567.7           0.20980   \n",
       "4    ...          16.67           152.20      1575.0           0.13740   \n",
       "..   ...            ...              ...         ...               ...   \n",
       "564  ...          26.40           166.10      2027.0           0.14100   \n",
       "565  ...          38.25           155.00      1731.0           0.11660   \n",
       "566  ...          34.12           126.70      1124.0           0.11390   \n",
       "567  ...          39.42           184.60      1821.0           0.16500   \n",
       "568  ...          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0              0.66560           0.7119                0.2654          0.4601   \n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "0                    0.11890          NaN  \n",
       "1                    0.08902          NaN  \n",
       "2                    0.08758          NaN  \n",
       "3                    0.17300          NaN  \n",
       "4                    0.07678          NaN  \n",
       "..                       ...          ...  \n",
       "564                  0.07115          NaN  \n",
       "565                  0.06637          NaN  \n",
       "566                  0.07820          NaN  \n",
       "567                  0.12400          NaN  \n",
       "568                  0.07039          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34fe9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([data.columns[0],data.columns[32]],axis =1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e663106",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis'].replace(to_replace= ['B','M'],value = [0,1],inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "409d2f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data.iloc[0:int(0.7*len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d9c0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data = data.iloc[int(0.7*len(data)):int(0.9*len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29eb9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = data.iloc[int(0.9*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d79eeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = training_data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e29a82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array(C).reshape(C.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83313109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4da07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(training_data['diagnosis']).reshape(training_data['diagnosis'].shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cb6f87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94962460",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= training_data.drop(['diagnosis'],inplace=False,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7094b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - np.mean(X,axis= 0))/np.std(X,axis=0)\n",
    "#it will convert it into zero mean one standard deviation so that gradient doesnt ovsershoot\n",
    "\n",
    "X_transpose = np.array(training_data.drop(['diagnosis'],axis =1))\n",
    "X_transpose.shape\n",
    "X_transpose = (X_transpose - np.mean(X_transpose,axis =0))/np.std(X_transpose,axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71fa2a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train  = X.shape[1]\n",
    "m=X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c838930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "used this in batch gradient descent\n",
    "def P(theta0,theta,X):\n",
    "    P_theta0_theta =  1/(1+ np.exp(-(theta0+np.matmul(X,theta)))) #got rank 1 matrix\n",
    "    P_theta0_theta = P_theta0_theta.values.reshape(P_theta0_theta.shape[0],1) #converting it into rank 2 matrix\n",
    "    return P_theta0_theta\n",
    "    \"\"\"\n",
    "def Posterior(theta0,theta,X_transpose):\n",
    "    discriminant_score = theta0 +np.matmul(X_transpose,theta)\n",
    "    P_theta0_theta = 1/(1+ np.exp(-discriminant_score))\n",
    "    return P_theta0_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7e7fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "used this in batch gradient descent\n",
    "def neg_log_loss(C,P_theta0_theta):\n",
    "    neg_log_loss_theta0_theta = -(np.matmul(C.T,np.log(P_theta0_theta))) +np.matmul((1-C).T,np.log(1-P_theta0_theta))\n",
    "    return neg_log_loss_theta0_theta\n",
    "    \"\"\"\n",
    "'''def neg_log_loss(train_labels,P_theta0_theta):\n",
    "    lhs_sum = np.matmul(train_labels.T,np.log(P_theta0_theta))\n",
    "    rhs_sum = np.matmul((1-train_labels).T,np.log(1-P_theta0_theta))\n",
    "    neg_log_loss_theta0_theta = -(1/train_labels.shape[0])*(lhs_sum+rhs_sum)\n",
    "    return neg_log_loss_theta0_theta\n",
    "'''\n",
    "#after regelarization\n",
    "def neg_log_loss(train_labels,P_theta0_theta):\n",
    "    lhs_sum = np.matmul(train_labels.T,np.log(P_theta0_theta))\n",
    "    rhs_sum = np.matmul((1-train_labels).T,np.log(1-P_theta0_theta))\n",
    "    neg_log_loss_theta0_theta = -(1/train_labels.shape[0])*(lhs_sum+rhs_sum)\n",
    "    return neg_log_loss_theta0_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3d923750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is called batch gradient descent\n",
    "\"\"\"\n",
    "theta0_initial = 0\n",
    "theta_initial = np.zeros((m,1))\n",
    "epsilon = 10**(-3)\n",
    "tol = 10**(-4)\n",
    "\n",
    "iterations = list()\n",
    "neg_log_loss_history = list()\n",
    "iteration_number = 0\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    P_initial =P(theta0_initial,theta_initial,X)\n",
    "    \n",
    "    theta0_final = theta0_initial - (epsilon*np.mean(P_initial - C))\n",
    "    theta_final = theta_initial - ((1/N_train)*epsilon*np.matmul((P_initial -C).T,X).T)\n",
    "    \n",
    "    neg_log_loss_initial = neg_log_loss(C,P_initial)\n",
    "    \n",
    "    P_final = P(theta0_final,theta_final,X)\n",
    "    \n",
    "    neg_log_loss_final = neg_log_loss(C,P_final)\n",
    "    \n",
    "    if abs(neg_log_loss_initial - neg_log_loss_final)< tol:\n",
    "        break\n",
    "    \n",
    "    theta0_initial = theta0_final\n",
    "    theta_initial = theta_final\n",
    "    \n",
    "    iterations.append(iteration_number)\n",
    "    neg_log_loss_history.append(neg_log_loss_initial)\n",
    "    \n",
    "    print(\"Iteration number - \", iteration_number, \"Cross Entropy loss - \", neg_log_loss_initial)\n",
    "    \n",
    "    iteration_number = iteration_number+1\n",
    "\"\"\"\n",
    "\n",
    "# below one it is called mini batch gradient descent\n",
    "# in Stochastic Gradient descent, mini batch size is 1\n",
    "theta0_initial = 0\n",
    "theta_initial = np.zeros((m,1))\n",
    "epsilon = 10**(-3)\n",
    "tol = 10**(-4)\n",
    "mini_batch_size=2\n",
    "time_steps = N_train//mini_batch_size\n",
    "\n",
    "\n",
    "iterations = list()\n",
    "neg_log_loss_history = list()\n",
    "iteration_number = 0\n",
    "epoc_counter = 0\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    \n",
    "    for i in range(0,time_steps):\n",
    "        rand_indices = np.random.choice(a = np.arange(0,N_train),size = mini_batch_size,replace=False)\n",
    "        X_transpose_mini_batch = X_transpose[rand_indices]\n",
    "        #X_transpose_mini_batch = X_transpose_mini_batch.reshape(-1,X_transpose_mini_batch.shape[0])\n",
    "        train_labels_mini_batch = train_labels[rand_indices]\n",
    "        #train_labels_mini_batch\n",
    "        \n",
    "        P_initial =Posterior(theta0_initial,theta_initial,X_transpose_mini_batch)\n",
    "\n",
    "        theta0_final = theta0_initial - (epsilon*np.mean(P_initial - train_labels_mini_batch))\n",
    "        theta_final = theta_initial - ((1/mini_batch_size)*epsilon*np.matmul((P_initial -train_labels_mini_batch).T,X_transpose_mini_batch).T)\n",
    "\n",
    "        neg_log_loss_initial = neg_log_loss(train_labels_mini_batch,P_initial)\n",
    "\n",
    "        P_final = Posterior(theta0_final,theta_final,X_transpose_mini_batch)\n",
    "\n",
    "        neg_log_loss_final = neg_log_loss(train_labels_mini_batch,P_final)\n",
    "\n",
    "        \n",
    "        theta0_initial = theta0_final\n",
    "        theta_initial = theta_final\n",
    "\n",
    "        iterations.append(iteration_number)\n",
    "        neg_log_loss_history.append(neg_log_loss_initial[0][0])\n",
    "\n",
    "        print(\"Iteration number - \", iteration_number, \"Cross Entropy loss - \", neg_log_loss_initial[0][0])\n",
    "\n",
    "        iteration_number = iteration_number+1\n",
    "        \n",
    "    epoch_counter = epoch_counter+1\n",
    "    \n",
    "    print(\"End of epoch number\", epoch_counter,\"cross entropy loss = \",neg_log_loss_initial[0][0])\n",
    "    \n",
    "    if abs(neg_log_loss_initial - neg_log_loss_final) < tol:\n",
    "        break\n",
    "\n",
    "def fit(step_size,reg_strength):\n",
    "    theta0_initial = 0\n",
    "    theta_initial = np.zeros((m,1))\n",
    "    epsilon = step_size\n",
    "    tol = 10**(-4)\n",
    "    mini_batch_size=2\n",
    "    time_steps = N_train//mini_batch_size\n",
    "\n",
    "\n",
    "    iterations = list()\n",
    "    neg_log_loss_history = list()\n",
    "    iteration_number = 0\n",
    "    epoc_counter = 0\n",
    "    invalid_value_flag = 0\n",
    "\n",
    "    while(True):\n",
    "\n",
    "\n",
    "        for i in range(0,time_steps):\n",
    "            rand_indices = np.random.choice(a = np.arange(0,N_train),size = mini_batch_size,replace=False)\n",
    "            X_transpose_mini_batch = X_transpose[rand_indices]\n",
    "            #X_transpose_mini_batch = X_transpose_mini_batch.reshape(-1,X_transpose_mini_batch.shape[0])\n",
    "            train_labels_mini_batch = train_labels[rand_indices]\n",
    "            #train_labels_mini_batch\n",
    "\n",
    "            P_initial =Posterior(theta0_initial,theta_initial,X_transpose_mini_batch)\n",
    "\n",
    "            theta0_final = theta0_initial - (epsilon*np.mean(P_initial - train_labels_mini_batch))\n",
    "            theta_final = theta_initial - ((1/mini_batch_size)*epsilon*np.matmul((P_initial -train_labels_mini_batch).T,X_transpose_mini_batch).T)\n",
    "\n",
    "            neg_log_loss_initial = neg_log_loss(train_labels_mini_batch,P_initial, theta_initial, reg_strength)\n",
    "\n",
    "            P_final = Posterior(theta0_final,theta_final,X_transpose_mini_batch)\n",
    "\n",
    "            neg_log_loss_final = neg_log_loss(train_labels_mini_batch,P_final,theta_final, reg_strength)\n",
    "            \n",
    "            if np.isnan(neg_log_loss_final[0][0]) == True:\n",
    "                invalid_value_flag = 1\n",
    "                break\n",
    "                \n",
    "            theta0_initial = theta0_final\n",
    "            theta_initial = theta_final\n",
    "\n",
    "            iterations.append(iteration_number)\n",
    "            neg_log_loss_history.append(neg_log_loss_initial[0][0])\n",
    "\n",
    "            print(\"Iteration number - \", iteration_number, \"Cross Entropy loss - \", neg_log_loss_initial[0][0])\n",
    "\n",
    "            iteration_number = iteration_number+1\n",
    "\n",
    "        epoch_counter = epoch_counter+1\n",
    "\n",
    "        print(\"End of epoch number\", epoch_counter,\"cross entropy loss = \",neg_log_loss_initial[0][0])\n",
    "\n",
    "        if abs(neg_log_loss_initial - neg_log_loss_final) < tol or invalid_value_flag == 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "801498a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_9260\\4120031933.py\", line 10, in <module>\n",
      "    grid_search_results.append(fit(step_size,reg_hyp_param))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_9260\\4034047251.py\", line 120, in fit\n",
      "    theta_final = theta_initial - ((1/mini_batch_size)*epsilon*np.matmul((P_initial -train_labels_mini_batch).T,X_transpose_mini_batch).T)\n",
      "                                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2016, in __array_ufunc__\n",
      "    return arraylike.array_ufunc(self, ufunc, method, *inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py\", line 273, in array_ufunc\n",
      "    result = maybe_dispatch_ufunc_to_dunder_op(self, ufunc, method, *inputs, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pandas\\_libs\\ops_dispatch.pyx\", line 107, in pandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\", line 1630, in __matmul__\n",
      "    return self.dot(other)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\", line 1583, in dot\n",
      "    raise ValueError(\"matrices are not aligned\")\n",
      "ValueError: matrices are not aligned\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1155, in get_records\n",
      "    FrameInfo(\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 780, in __init__\n",
      "    ix = inspect.getsourcelines(frame)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\inspect.py\", line 1244, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\inspect.py\", line 1081, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(theta0,theta,X_transpose_labels) : \n",
    "    P_mini_batch = Posterior(theta0,theta,X_transpose)\n",
    "    Predicted_labels = P_mini_batch>0.5\n",
    "    accuracy = np.count_nonzero(Predicted_labels == labels)/labels.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "grid_search_results = list()\n",
    "for step_size in [0.001,0.01]:\n",
    "    for reg_hyp_param in [0.0001,0.001,0.01,0.1,1,10,100]:\n",
    "        grid_search_results.append(fit(step_size,reg_hyp_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1c3aedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv_results = dict()\n",
    "for configs in grid_search_results:\n",
    "    grid_search_cv_results[tuple(configs.keys())] = compute_accuracy(list(configs.values())[0][0],\n",
    "                                                                    list(configs.values())[0][1],\n",
    "                                                                     X_cv_transpose,cv_labels\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c6d8552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11633784],\n",
       "       [ 0.08942636],\n",
       "       [ 0.11930624],\n",
       "       [ 0.10525554],\n",
       "       [ 0.06158533],\n",
       "       [ 0.11125938],\n",
       "       [ 0.10043878],\n",
       "       [ 0.12608506],\n",
       "       [ 0.05928491],\n",
       "       [ 0.02992055],\n",
       "       [ 0.08841461],\n",
       "       [-0.01355317],\n",
       "       [ 0.08972825],\n",
       "       [ 0.07817057],\n",
       "       [-0.02807325],\n",
       "       [ 0.04207264],\n",
       "       [ 0.02276156],\n",
       "       [ 0.06781886],\n",
       "       [-0.0214928 ],\n",
       "       [ 0.03192627],\n",
       "       [ 0.14033287],\n",
       "       [ 0.11599311],\n",
       "       [ 0.14414587],\n",
       "       [ 0.12658993],\n",
       "       [ 0.09086682],\n",
       "       [ 0.13130302],\n",
       "       [ 0.11211177],\n",
       "       [ 0.16357462],\n",
       "       [ 0.10748125],\n",
       "       [ 0.12944173]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b66e6d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>11.06</td>\n",
       "      <td>14.83</td>\n",
       "      <td>70.31</td>\n",
       "      <td>378.2</td>\n",
       "      <td>0.07741</td>\n",
       "      <td>0.04768</td>\n",
       "      <td>0.02712</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.1535</td>\n",
       "      <td>...</td>\n",
       "      <td>12.68</td>\n",
       "      <td>20.35</td>\n",
       "      <td>80.79</td>\n",
       "      <td>496.7</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>0.05556</td>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.09158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>11.80</td>\n",
       "      <td>17.26</td>\n",
       "      <td>75.26</td>\n",
       "      <td>431.9</td>\n",
       "      <td>0.09087</td>\n",
       "      <td>0.06232</td>\n",
       "      <td>0.02853</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>...</td>\n",
       "      <td>13.45</td>\n",
       "      <td>24.49</td>\n",
       "      <td>86.00</td>\n",
       "      <td>562.0</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.05356</td>\n",
       "      <td>0.2779</td>\n",
       "      <td>0.08121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1</td>\n",
       "      <td>17.91</td>\n",
       "      <td>21.02</td>\n",
       "      <td>124.40</td>\n",
       "      <td>994.0</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.31890</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>0.2113</td>\n",
       "      <td>...</td>\n",
       "      <td>20.80</td>\n",
       "      <td>27.78</td>\n",
       "      <td>149.60</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>0.18730</td>\n",
       "      <td>0.5917</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>0.19640</td>\n",
       "      <td>0.3245</td>\n",
       "      <td>0.11980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>10.91</td>\n",
       "      <td>76.14</td>\n",
       "      <td>442.7</td>\n",
       "      <td>0.08872</td>\n",
       "      <td>0.05242</td>\n",
       "      <td>0.02606</td>\n",
       "      <td>0.017960</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>...</td>\n",
       "      <td>13.80</td>\n",
       "      <td>20.14</td>\n",
       "      <td>87.64</td>\n",
       "      <td>589.5</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.1575</td>\n",
       "      <td>0.1514</td>\n",
       "      <td>0.06876</td>\n",
       "      <td>0.2460</td>\n",
       "      <td>0.07262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0</td>\n",
       "      <td>12.96</td>\n",
       "      <td>18.29</td>\n",
       "      <td>84.18</td>\n",
       "      <td>525.2</td>\n",
       "      <td>0.07351</td>\n",
       "      <td>0.07899</td>\n",
       "      <td>0.04057</td>\n",
       "      <td>0.018830</td>\n",
       "      <td>0.1874</td>\n",
       "      <td>...</td>\n",
       "      <td>14.13</td>\n",
       "      <td>24.61</td>\n",
       "      <td>96.31</td>\n",
       "      <td>621.9</td>\n",
       "      <td>0.09329</td>\n",
       "      <td>0.2318</td>\n",
       "      <td>0.1604</td>\n",
       "      <td>0.06608</td>\n",
       "      <td>0.3207</td>\n",
       "      <td>0.07247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>0</td>\n",
       "      <td>11.06</td>\n",
       "      <td>17.12</td>\n",
       "      <td>71.25</td>\n",
       "      <td>366.5</td>\n",
       "      <td>0.11940</td>\n",
       "      <td>0.10710</td>\n",
       "      <td>0.04063</td>\n",
       "      <td>0.042680</td>\n",
       "      <td>0.1954</td>\n",
       "      <td>...</td>\n",
       "      <td>11.69</td>\n",
       "      <td>20.74</td>\n",
       "      <td>76.08</td>\n",
       "      <td>411.1</td>\n",
       "      <td>0.16620</td>\n",
       "      <td>0.2031</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.09514</td>\n",
       "      <td>0.2780</td>\n",
       "      <td>0.11680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>0</td>\n",
       "      <td>16.30</td>\n",
       "      <td>15.70</td>\n",
       "      <td>104.70</td>\n",
       "      <td>819.8</td>\n",
       "      <td>0.09427</td>\n",
       "      <td>0.06712</td>\n",
       "      <td>0.05526</td>\n",
       "      <td>0.045630</td>\n",
       "      <td>0.1711</td>\n",
       "      <td>...</td>\n",
       "      <td>17.32</td>\n",
       "      <td>17.76</td>\n",
       "      <td>109.80</td>\n",
       "      <td>928.2</td>\n",
       "      <td>0.13540</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.1947</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.07230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>1</td>\n",
       "      <td>15.46</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.80</td>\n",
       "      <td>731.3</td>\n",
       "      <td>0.11830</td>\n",
       "      <td>0.18700</td>\n",
       "      <td>0.20300</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.1807</td>\n",
       "      <td>...</td>\n",
       "      <td>17.11</td>\n",
       "      <td>36.33</td>\n",
       "      <td>117.70</td>\n",
       "      <td>909.4</td>\n",
       "      <td>0.17320</td>\n",
       "      <td>0.4967</td>\n",
       "      <td>0.5911</td>\n",
       "      <td>0.21630</td>\n",
       "      <td>0.3013</td>\n",
       "      <td>0.10670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>0</td>\n",
       "      <td>11.74</td>\n",
       "      <td>14.69</td>\n",
       "      <td>76.31</td>\n",
       "      <td>426.0</td>\n",
       "      <td>0.08099</td>\n",
       "      <td>0.09661</td>\n",
       "      <td>0.06726</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.1499</td>\n",
       "      <td>...</td>\n",
       "      <td>12.45</td>\n",
       "      <td>17.60</td>\n",
       "      <td>81.25</td>\n",
       "      <td>473.8</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.2793</td>\n",
       "      <td>0.2690</td>\n",
       "      <td>0.10560</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.09879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>0</td>\n",
       "      <td>14.81</td>\n",
       "      <td>14.70</td>\n",
       "      <td>94.66</td>\n",
       "      <td>680.7</td>\n",
       "      <td>0.08472</td>\n",
       "      <td>0.05016</td>\n",
       "      <td>0.03416</td>\n",
       "      <td>0.025410</td>\n",
       "      <td>0.1659</td>\n",
       "      <td>...</td>\n",
       "      <td>15.61</td>\n",
       "      <td>17.58</td>\n",
       "      <td>101.70</td>\n",
       "      <td>760.2</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.1011</td>\n",
       "      <td>0.1101</td>\n",
       "      <td>0.07955</td>\n",
       "      <td>0.2334</td>\n",
       "      <td>0.06142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "398          0        11.06         14.83           70.31      378.2   \n",
       "399          0        11.80         17.26           75.26      431.9   \n",
       "400          1        17.91         21.02          124.40      994.0   \n",
       "401          0        11.93         10.91           76.14      442.7   \n",
       "402          0        12.96         18.29           84.18      525.2   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "507          0        11.06         17.12           71.25      366.5   \n",
       "508          0        16.30         15.70          104.70      819.8   \n",
       "509          1        15.46         23.95          103.80      731.3   \n",
       "510          0        11.74         14.69           76.31      426.0   \n",
       "511          0        14.81         14.70           94.66      680.7   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "398          0.07741           0.04768         0.02712             0.007246   \n",
       "399          0.09087           0.06232         0.02853             0.016380   \n",
       "400          0.12300           0.25760         0.31890             0.119800   \n",
       "401          0.08872           0.05242         0.02606             0.017960   \n",
       "402          0.07351           0.07899         0.04057             0.018830   \n",
       "..               ...               ...             ...                  ...   \n",
       "507          0.11940           0.10710         0.04063             0.042680   \n",
       "508          0.09427           0.06712         0.05526             0.045630   \n",
       "509          0.11830           0.18700         0.20300             0.085200   \n",
       "510          0.08099           0.09661         0.06726             0.026390   \n",
       "511          0.08472           0.05016         0.03416             0.025410   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "398         0.1535  ...         12.68          20.35            80.79   \n",
       "399         0.1847  ...         13.45          24.49            86.00   \n",
       "400         0.2113  ...         20.80          27.78           149.60   \n",
       "401         0.1601  ...         13.80          20.14            87.64   \n",
       "402         0.1874  ...         14.13          24.61            96.31   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "507         0.1954  ...         11.69          20.74            76.08   \n",
       "508         0.1711  ...         17.32          17.76           109.80   \n",
       "509         0.1807  ...         17.11          36.33           117.70   \n",
       "510         0.1499  ...         12.45          17.60            81.25   \n",
       "511         0.1659  ...         15.61          17.58           101.70   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "398       496.7           0.11200             0.1879           0.2079   \n",
       "399       562.0           0.12440             0.1726           0.1449   \n",
       "400      1304.0           0.18730             0.5917           0.9034   \n",
       "401       589.5           0.13740             0.1575           0.1514   \n",
       "402       621.9           0.09329             0.2318           0.1604   \n",
       "..          ...               ...                ...              ...   \n",
       "507       411.1           0.16620             0.2031           0.1256   \n",
       "508       928.2           0.13540             0.1361           0.1947   \n",
       "509       909.4           0.17320             0.4967           0.5911   \n",
       "510       473.8           0.10730             0.2793           0.2690   \n",
       "511       760.2           0.11390             0.1011           0.1101   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "398               0.05556          0.2590                  0.09158  \n",
       "399               0.05356          0.2779                  0.08121  \n",
       "400               0.19640          0.3245                  0.11980  \n",
       "401               0.06876          0.2460                  0.07262  \n",
       "402               0.06608          0.3207                  0.07247  \n",
       "..                    ...             ...                      ...  \n",
       "507               0.09514          0.2780                  0.11680  \n",
       "508               0.13570          0.2300                  0.07230  \n",
       "509               0.21630          0.3013                  0.10670  \n",
       "510               0.10560          0.2604                  0.09879  \n",
       "511               0.07955          0.2334                  0.06142  \n",
       "\n",
       "[114 rows x 31 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5cbede34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_gt_labels = np.array(cv_data['diagnosis']).reshape(cv_data.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "067029c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_9260\\1585920398.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cv_data.drop(['diagnosis'],axis =1, inplace =True)\n"
     ]
    }
   ],
   "source": [
    "cv_data.drop(['diagnosis'],axis =1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92a1e3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>11.06</td>\n",
       "      <td>14.83</td>\n",
       "      <td>70.31</td>\n",
       "      <td>378.2</td>\n",
       "      <td>0.07741</td>\n",
       "      <td>0.04768</td>\n",
       "      <td>0.02712</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.1535</td>\n",
       "      <td>0.06214</td>\n",
       "      <td>...</td>\n",
       "      <td>12.68</td>\n",
       "      <td>20.35</td>\n",
       "      <td>80.79</td>\n",
       "      <td>496.7</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>0.05556</td>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.09158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>11.80</td>\n",
       "      <td>17.26</td>\n",
       "      <td>75.26</td>\n",
       "      <td>431.9</td>\n",
       "      <td>0.09087</td>\n",
       "      <td>0.06232</td>\n",
       "      <td>0.02853</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.06019</td>\n",
       "      <td>...</td>\n",
       "      <td>13.45</td>\n",
       "      <td>24.49</td>\n",
       "      <td>86.00</td>\n",
       "      <td>562.0</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.05356</td>\n",
       "      <td>0.2779</td>\n",
       "      <td>0.08121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>17.91</td>\n",
       "      <td>21.02</td>\n",
       "      <td>124.40</td>\n",
       "      <td>994.0</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.31890</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>0.2113</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>...</td>\n",
       "      <td>20.80</td>\n",
       "      <td>27.78</td>\n",
       "      <td>149.60</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>0.18730</td>\n",
       "      <td>0.5917</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>0.19640</td>\n",
       "      <td>0.3245</td>\n",
       "      <td>0.11980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>11.93</td>\n",
       "      <td>10.91</td>\n",
       "      <td>76.14</td>\n",
       "      <td>442.7</td>\n",
       "      <td>0.08872</td>\n",
       "      <td>0.05242</td>\n",
       "      <td>0.02606</td>\n",
       "      <td>0.017960</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.05541</td>\n",
       "      <td>...</td>\n",
       "      <td>13.80</td>\n",
       "      <td>20.14</td>\n",
       "      <td>87.64</td>\n",
       "      <td>589.5</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.1575</td>\n",
       "      <td>0.1514</td>\n",
       "      <td>0.06876</td>\n",
       "      <td>0.2460</td>\n",
       "      <td>0.07262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>12.96</td>\n",
       "      <td>18.29</td>\n",
       "      <td>84.18</td>\n",
       "      <td>525.2</td>\n",
       "      <td>0.07351</td>\n",
       "      <td>0.07899</td>\n",
       "      <td>0.04057</td>\n",
       "      <td>0.018830</td>\n",
       "      <td>0.1874</td>\n",
       "      <td>0.05899</td>\n",
       "      <td>...</td>\n",
       "      <td>14.13</td>\n",
       "      <td>24.61</td>\n",
       "      <td>96.31</td>\n",
       "      <td>621.9</td>\n",
       "      <td>0.09329</td>\n",
       "      <td>0.2318</td>\n",
       "      <td>0.1604</td>\n",
       "      <td>0.06608</td>\n",
       "      <td>0.3207</td>\n",
       "      <td>0.07247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>11.06</td>\n",
       "      <td>17.12</td>\n",
       "      <td>71.25</td>\n",
       "      <td>366.5</td>\n",
       "      <td>0.11940</td>\n",
       "      <td>0.10710</td>\n",
       "      <td>0.04063</td>\n",
       "      <td>0.042680</td>\n",
       "      <td>0.1954</td>\n",
       "      <td>0.07976</td>\n",
       "      <td>...</td>\n",
       "      <td>11.69</td>\n",
       "      <td>20.74</td>\n",
       "      <td>76.08</td>\n",
       "      <td>411.1</td>\n",
       "      <td>0.16620</td>\n",
       "      <td>0.2031</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.09514</td>\n",
       "      <td>0.2780</td>\n",
       "      <td>0.11680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>16.30</td>\n",
       "      <td>15.70</td>\n",
       "      <td>104.70</td>\n",
       "      <td>819.8</td>\n",
       "      <td>0.09427</td>\n",
       "      <td>0.06712</td>\n",
       "      <td>0.05526</td>\n",
       "      <td>0.045630</td>\n",
       "      <td>0.1711</td>\n",
       "      <td>0.05657</td>\n",
       "      <td>...</td>\n",
       "      <td>17.32</td>\n",
       "      <td>17.76</td>\n",
       "      <td>109.80</td>\n",
       "      <td>928.2</td>\n",
       "      <td>0.13540</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.1947</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.07230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>15.46</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.80</td>\n",
       "      <td>731.3</td>\n",
       "      <td>0.11830</td>\n",
       "      <td>0.18700</td>\n",
       "      <td>0.20300</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.1807</td>\n",
       "      <td>0.07083</td>\n",
       "      <td>...</td>\n",
       "      <td>17.11</td>\n",
       "      <td>36.33</td>\n",
       "      <td>117.70</td>\n",
       "      <td>909.4</td>\n",
       "      <td>0.17320</td>\n",
       "      <td>0.4967</td>\n",
       "      <td>0.5911</td>\n",
       "      <td>0.21630</td>\n",
       "      <td>0.3013</td>\n",
       "      <td>0.10670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>11.74</td>\n",
       "      <td>14.69</td>\n",
       "      <td>76.31</td>\n",
       "      <td>426.0</td>\n",
       "      <td>0.08099</td>\n",
       "      <td>0.09661</td>\n",
       "      <td>0.06726</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.1499</td>\n",
       "      <td>0.06758</td>\n",
       "      <td>...</td>\n",
       "      <td>12.45</td>\n",
       "      <td>17.60</td>\n",
       "      <td>81.25</td>\n",
       "      <td>473.8</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.2793</td>\n",
       "      <td>0.2690</td>\n",
       "      <td>0.10560</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.09879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>14.81</td>\n",
       "      <td>14.70</td>\n",
       "      <td>94.66</td>\n",
       "      <td>680.7</td>\n",
       "      <td>0.08472</td>\n",
       "      <td>0.05016</td>\n",
       "      <td>0.03416</td>\n",
       "      <td>0.025410</td>\n",
       "      <td>0.1659</td>\n",
       "      <td>0.05348</td>\n",
       "      <td>...</td>\n",
       "      <td>15.61</td>\n",
       "      <td>17.58</td>\n",
       "      <td>101.70</td>\n",
       "      <td>760.2</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.1011</td>\n",
       "      <td>0.1101</td>\n",
       "      <td>0.07955</td>\n",
       "      <td>0.2334</td>\n",
       "      <td>0.06142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "398        11.06         14.83           70.31      378.2          0.07741   \n",
       "399        11.80         17.26           75.26      431.9          0.09087   \n",
       "400        17.91         21.02          124.40      994.0          0.12300   \n",
       "401        11.93         10.91           76.14      442.7          0.08872   \n",
       "402        12.96         18.29           84.18      525.2          0.07351   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "507        11.06         17.12           71.25      366.5          0.11940   \n",
       "508        16.30         15.70          104.70      819.8          0.09427   \n",
       "509        15.46         23.95          103.80      731.3          0.11830   \n",
       "510        11.74         14.69           76.31      426.0          0.08099   \n",
       "511        14.81         14.70           94.66      680.7          0.08472   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "398           0.04768         0.02712             0.007246         0.1535   \n",
       "399           0.06232         0.02853             0.016380         0.1847   \n",
       "400           0.25760         0.31890             0.119800         0.2113   \n",
       "401           0.05242         0.02606             0.017960         0.1601   \n",
       "402           0.07899         0.04057             0.018830         0.1874   \n",
       "..                ...             ...                  ...            ...   \n",
       "507           0.10710         0.04063             0.042680         0.1954   \n",
       "508           0.06712         0.05526             0.045630         0.1711   \n",
       "509           0.18700         0.20300             0.085200         0.1807   \n",
       "510           0.09661         0.06726             0.026390         0.1499   \n",
       "511           0.05016         0.03416             0.025410         0.1659   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "398                 0.06214  ...         12.68          20.35   \n",
       "399                 0.06019  ...         13.45          24.49   \n",
       "400                 0.07115  ...         20.80          27.78   \n",
       "401                 0.05541  ...         13.80          20.14   \n",
       "402                 0.05899  ...         14.13          24.61   \n",
       "..                      ...  ...           ...            ...   \n",
       "507                 0.07976  ...         11.69          20.74   \n",
       "508                 0.05657  ...         17.32          17.76   \n",
       "509                 0.07083  ...         17.11          36.33   \n",
       "510                 0.06758  ...         12.45          17.60   \n",
       "511                 0.05348  ...         15.61          17.58   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "398            80.79       496.7           0.11200             0.1879   \n",
       "399            86.00       562.0           0.12440             0.1726   \n",
       "400           149.60      1304.0           0.18730             0.5917   \n",
       "401            87.64       589.5           0.13740             0.1575   \n",
       "402            96.31       621.9           0.09329             0.2318   \n",
       "..               ...         ...               ...                ...   \n",
       "507            76.08       411.1           0.16620             0.2031   \n",
       "508           109.80       928.2           0.13540             0.1361   \n",
       "509           117.70       909.4           0.17320             0.4967   \n",
       "510            81.25       473.8           0.10730             0.2793   \n",
       "511           101.70       760.2           0.11390             0.1011   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "398           0.2079               0.05556          0.2590   \n",
       "399           0.1449               0.05356          0.2779   \n",
       "400           0.9034               0.19640          0.3245   \n",
       "401           0.1514               0.06876          0.2460   \n",
       "402           0.1604               0.06608          0.3207   \n",
       "..               ...                   ...             ...   \n",
       "507           0.1256               0.09514          0.2780   \n",
       "508           0.1947               0.13570          0.2300   \n",
       "509           0.5911               0.21630          0.3013   \n",
       "510           0.2690               0.10560          0.2604   \n",
       "511           0.1101               0.07955          0.2334   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "398                  0.09158  \n",
       "399                  0.08121  \n",
       "400                  0.11980  \n",
       "401                  0.07262  \n",
       "402                  0.07247  \n",
       "..                       ...  \n",
       "507                  0.11680  \n",
       "508                  0.07230  \n",
       "509                  0.10670  \n",
       "510                  0.09879  \n",
       "511                  0.06142  \n",
       "\n",
       "[114 rows x 30 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "57717817",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv_data = np.array(cv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f9b3917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv_data = (X_cv_data - np.mean(X_cv_data,axis=0))/np.std(X_cv_data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "def46d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 114)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_final = theta_final.T\n",
    "X_cv_data =X_cv_data.T\n",
    "X_cv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "08e91317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17671207],\n",
       "       [0.26601289],\n",
       "       [0.98836305],\n",
       "       [0.18767243],\n",
       "       [0.33568448],\n",
       "       [0.32805234],\n",
       "       [0.12601729],\n",
       "       [0.26113432],\n",
       "       [0.4892713 ],\n",
       "       [0.36397734],\n",
       "       [0.91162361],\n",
       "       [0.35944306],\n",
       "       [0.29581517],\n",
       "       [0.25826199],\n",
       "       [0.14423512],\n",
       "       [0.70332242],\n",
       "       [0.51187142],\n",
       "       [0.35503327],\n",
       "       [0.22352703],\n",
       "       [0.97730873],\n",
       "       [0.25084885],\n",
       "       [0.19646722],\n",
       "       [0.34353521],\n",
       "       [0.77829575],\n",
       "       [0.37570703],\n",
       "       [0.60789352],\n",
       "       [0.2884602 ],\n",
       "       [0.10965905],\n",
       "       [0.41466123],\n",
       "       [0.31992739],\n",
       "       [0.09566003],\n",
       "       [0.12973121],\n",
       "       [0.9715127 ],\n",
       "       [0.45098089],\n",
       "       [0.95843656],\n",
       "       [0.95044184],\n",
       "       [0.32328675],\n",
       "       [0.80190861],\n",
       "       [0.3403493 ],\n",
       "       [0.31299056],\n",
       "       [0.27202834],\n",
       "       [0.20492424],\n",
       "       [0.57268461],\n",
       "       [0.88637509],\n",
       "       [0.16391522],\n",
       "       [0.11917901],\n",
       "       [0.78788006],\n",
       "       [0.45398123],\n",
       "       [0.94657125],\n",
       "       [0.45479433],\n",
       "       [0.47449114],\n",
       "       [0.96006075],\n",
       "       [0.42846585],\n",
       "       [0.86665627],\n",
       "       [0.34935846],\n",
       "       [0.43579437],\n",
       "       [0.29194243],\n",
       "       [0.41554951],\n",
       "       [0.43944831],\n",
       "       [0.28636448],\n",
       "       [0.23591573],\n",
       "       [0.1527606 ],\n",
       "       [0.93266664],\n",
       "       [0.99986991],\n",
       "       [0.30715462],\n",
       "       [0.25844237],\n",
       "       [0.27781513],\n",
       "       [0.81214894],\n",
       "       [0.57589485],\n",
       "       [0.1445027 ],\n",
       "       [0.97709012],\n",
       "       [0.66217273],\n",
       "       [0.25535213],\n",
       "       [0.30206845],\n",
       "       [0.47024514],\n",
       "       [0.16278987],\n",
       "       [0.37941892],\n",
       "       [0.37451269],\n",
       "       [0.58016338],\n",
       "       [0.24525928],\n",
       "       [0.32090594],\n",
       "       [0.92790386],\n",
       "       [0.24857628],\n",
       "       [0.31757649],\n",
       "       [0.50309276],\n",
       "       [0.33113404],\n",
       "       [0.63029727],\n",
       "       [0.80137858],\n",
       "       [0.33723192],\n",
       "       [0.97029953],\n",
       "       [0.35704344],\n",
       "       [0.65760569],\n",
       "       [0.30445226],\n",
       "       [0.3558559 ],\n",
       "       [0.89622615],\n",
       "       [0.09969311],\n",
       "       [0.25306644],\n",
       "       [0.41698159],\n",
       "       [0.65782765],\n",
       "       [0.33233457],\n",
       "       [0.92676414],\n",
       "       [0.97314091],\n",
       "       [0.5411872 ],\n",
       "       [0.91261252],\n",
       "       [0.39798308],\n",
       "       [0.9893723 ],\n",
       "       [0.71776693],\n",
       "       [0.68508496],\n",
       "       [0.45998742],\n",
       "       [0.43794709],\n",
       "       [0.46066871],\n",
       "       [0.95332348],\n",
       "       [0.33366741],\n",
       "       [0.23479328]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_pred = Posterior(theta0_final,theta_final,X_cv_data)\n",
    "cv_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "16bfebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred = cv_pred>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "28dfeabd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cv_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "cv_pred.shape[0]['True']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1ffc0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.count_nonzero(cv_pred == cv_gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "64c49b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b3f05f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_accuracy = accuracy/X_cv_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6152ce3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_accuracy #above code method is batch gradient descent and in this accuracu, decision boundary comes same almost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "016627a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5942"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e2edf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff72946e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b9f1955390>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iterations[0],neg_log_loss_history[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46e645",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#it is called mini batch gradient descent\n",
    "theta0_initial = 0\n",
    "theta_initial = np.zeros((m,1))\n",
    "epsilon = 10**(-3)\n",
    "tol = 10**(-4)\n",
    "mini_batch_size=2\n",
    "time_steps = N_train//mini_batch_size\n",
    "\n",
    "\n",
    "iterations = list()\n",
    "neg_log_loss_history = list()\n",
    "iteration_number = 0\n",
    "epoc_counter = 0\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    \n",
    "    for i in range(0,time_steps):\n",
    "        rand_indices = np.random.choice(a = np.arange(0,N_train),size = mini_batch_size,replace=False)\n",
    "        X_transpose_mini_batch = X_transpose[rand_indices]\n",
    "        train_labels_mini_batch = train_labels[rand_indices]\n",
    "        \n",
    "        P_initial =P(theta0_initial,theta_initial,X_transpose_mini_batch)\n",
    "\n",
    "        theta0_final = theta0_initial - (epsilon*np.mean(P_initial - train_labels_mini_batch))\n",
    "        theta_final = theta_initial - ((1/N_train)*epsilon*np.matmul((P_initial -train_labels_mini_batch).T,X_transpose_mini_batch).T)\n",
    "\n",
    "        neg_log_loss_initial = neg_log_loss(train_labels_mini_batch,P_initial)\n",
    "\n",
    "        P_final = P(theta0_final,theta_final,X_transpose_mini_batch)\n",
    "\n",
    "        neg_log_loss_final = neg_log_loss(train_labels_mini_batch,P_final)\n",
    "\n",
    "        \n",
    "        theta0_initial = theta0_final\n",
    "        theta_initial = theta_final\n",
    "\n",
    "        iterations.append(iteration_number)\n",
    "        neg_log_loss_history.append(neg_log_loss_initial[0][0])\n",
    "\n",
    "        print(\"Iteration number - \", iteration_number, \"Cross Entropy loss - \", neg_log_loss_initial[0][0])\n",
    "\n",
    "        iteration_number = iteration_number+1\n",
    "        \n",
    "    epoch_counter = epoch_counter+1\n",
    "    \n",
    "    print(\"End of epoch number\", epoch_counter,\"cross entropy loss = \",neg_log_loss_initial[0][0])\n",
    "    \n",
    "    if abs(neg_log_loss_initial - neg_log_loss_final) < tol:\n",
    "        break\n",
    "    \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
